---
title: "Impagliazzo's Five Worlds, or The Computational (Im)Possibilities of The World That We Live In"
layout: post
tags: [theory]
cover: assets/images/posts/denali.jpg
class: post-template
navigation: True
author: fanpu
toc: true
---

<div>
Most people have probably heard of the \(\P = \NP? \) problem in some shape or
form, which asks whether the class of languages decidable in deterministic
polynomial time is the same as the class of languages decidable in
non-deterministic polynomial time. An affirmative answer in either direction
will have profound implications on complexity theory and on our world, as we will soon see.

<br>
<br>

It is generally believed that \( \P \neq \NP \) among complexity theorists, and
in fact, it is not uncommon for many theorems to make use of even stronger
assumptions such as the Exponential Time Hypothesis (ETH) 
in order to push some theorems through.
<br>
<br>

In this post, we extend the discussion to other possible possible outcomes based on Impagliazzo's influential 1995 paper [A personal view of average-case complexity](https://ieeexplore.ieee.org/document/514853).

</div>

# A Primer on Complexity Classes

<div>

\( BPP \) (bounded-error probabilistic polynomial time) is the class of
languages that can be solved in polynomial time by a Turing machine that has
access to random coins, and returns results with a constant error (say 1/3 of
the time). The exact constant does not matter since execution can be simply
repeated an additional polynomial number of times to bring the error down exponentially, without
changing the total polytime execution time. A language in \(BPP\) means that we
can have a very efficient algorithm in practice since the error can 

</div>

# The Five Worlds

<div>

Here, we discuss each of the Five Worlds - Algorithmica, Heuristica, Pessiland, Minicrypt, and Cryptomania, Strictly speaking, there are other intermediate possibilities that Impagliazzo excluded which Arora and Barak calls "Weirdland", such as if \( \SAT \) takes superpolynomial time to solve, or the complexity of \( \SAT \) differs wildly depending on the input size.

</div>

## Algorithmica

<div>

In Algorithmica, either \(P = NP\) or some moral equivalent like \(NP \subseteq
BPP\) holds true.  This means that whenever we have a problem where it is easy
to check whether a solution is valid (meaning it is in \( NP\)), we also have an
algorithm to efficiently recover the solution.

<br> <br>

A major implication is that almost all optimization problems become easy:
instead of maximizing an objective function \(f\), we can reduce the problem to
asking if there exists a solution \( (x_1, x_2, \cdots, x_n) \) that results in
an objective value \( f(x_1, x_2, \cdots, x_n) \geq c \) for some threshold
\(c\).  Verifying whether the solution meets the threshold is easy and therefore
in \(\NP\), which admits an efficient algorithm under our Algorithmica
assumption.  Binary search can be performed on \(c\) to find a solution with the
best value.  This will bring paradise to many areas of engineering and science.

<br>
<br>

On the other hand, cryptography is now dead. Cryptography is founded on the
assumption of one-way functions, where given some function \( f \), it is easy
to compute \( y = f ( x )) \) given \( x \) but hard to invert \( f \) to
recover \( x \) given \( y \). This is because one-way functions are in \( \NP
\), which obliterates this assumption.

</div>

## Heuristica

<div>

In Heuristica, problems in \( \NP \) have worst-case hardness, but all problems
that we encounter in practice can be solved in polynomial time.  To make it
precise, we can think of inputs to all our problems as being coming from a
probability distribution that is generated by an algorithm that runs in
polytime. This could be an algorithm that computes the cumulative distribution
function for the distribution.  The constrained running time of this algorithm
therefore limits how adversarial the probability distribution can be.

<br> <br>

Levin (of Cook-Levin fame) argues that if we believe in the strong Church-Turing
thesis, which states that the universe is essentially a Turing machine, then all
problems that we encounter in real life must be generated by something like a
polytime Turing machine. Therefore, \( \NP \) problems are easy on average, or
have polynomial average-case complexity.

In computational complexity literature, this is saying that \( \distNP \subseteq
\distP \).

<br> <br>

In addition, cryptography is still dead, because it is extremely hard to come up
with any puzzles that an adversary can't solve efficiently. 

</div>

## Pessiland

<div>

Pessiland is named as such as it is the worst possible world, where problems in
\( \NP \) are not easy on average, and one-way functions also do not exist. 

<br> <br>

This means that it is easy to come up with many hard unsolved instances of
problems (perhaps by randomness), but hard to come up with solved instances that
could be used for one-way functions. 

<br> <br>

In such a world, applying generic algorithms to solve problems will not take one
very far, and most advances will be made by understanding the specific problem
domain better and by making tailored optimizations, not very much unlike what we
currently do in our world. 

</div>

## Minicrypt

<div>

In Minicrypt, one-way functions exist, but not public key cryptography. One of
the two most popular primitives used in public-key cryptography protocols are
the Diffie-Hellman key-exchange protocol, and the RSA asymmetric encryption
algorithm.

<br> <br>

The Diffie-Hellman key exchange protocol is one of the most widespread protocols
used for performing a secret key exchange, and it relies on the Decisional
Diffie-Hellman (DDH) assumption. The DDH states that given any cyclic group \( G
\) with generator \( g \) and order \( q \), the following two probability
distributions are computationally indistinguishable:

$$ ( g^a, g^b, g^{ab})$$
$$ ( g^a, g^b, g^c) $$

where \( a, b, c \in \mathbb{Z}_q \).

<br>
<br>

RSA encryption relies on the assumption of the hardness of factorization, which
is already untrue in the quantum computation model. This is thanks to Shor's
algorithm developed in 1994 by Peter Shor, which is able to perform integer
factorization in \( \BQP \), i.e in polynomial time on a quantum computer.  This
has also caused a general shift among the cryptography community to start moving
standards towards post-quantum cryptography, which relies on cryptosystems that are
believed to be resistant to quantum computers.

</div>

## Cryptomania

<div>

Finally, Cryptomania is the world that most computational complexity experts
believe that we live in. Public key cryptography exists (which implies the
existence of one-way functions), allowing for many exciting applications such as
secure multi-party computation and electronic money systems like Bitcoin. Not
too bad of a trade, if we consider the downsides of losing the ability to solve
\( \NP \) problems efficiently on average!

</div>

# Closing Thoughts

I found it illuminating to consider a more nuanced debate of the computational
possibilities of our world beyond just a simplistic \( \P = \NP? \) view. Most
experts think that we are in Minicrypt or Cryptomania. It would be interesting
to see if the cryptosystems that we rely so heavily on today (Diffie-Hellman,
elliptic curve cryptography, etc) will be broken in the coming decades, since we
have no real reason to believe that the assumptions are hard other than the fact
that a ton of smart people have tried for years to break it and failed.


# References

- [Personal View of Average-Case Complexity](https://www2.karlin.mff.cuni.cz/~krajicek/ri5svetu.pdf)

- [Average Case Complexity](
https://www2.cs.sfu.ca/~kabanets/881/scribe_notes/lec8.pdf)

- Arora, S., &amp; Barak, B. (2016). Computational Complexity: A Modern Approach. Cambridge University Press. 

*Banner picture: Denali National Park, Alaska*
